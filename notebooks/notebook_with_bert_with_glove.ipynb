{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "hMhOPhNGhgr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "MhID41WnkFEJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Z_rOgjIQ9e",
        "outputId": "923b2881-ea17-4a8d-a1b9-c744cd365663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/sentiment_data/trainingandtestdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiKvaXMcIUIg",
        "outputId": "38b94bdf-3928-47e6-aa9b-743566b2e6e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/sentiment_data/trainingandtestdata'\n",
            "/content/drive/MyDrive/sentiment_data/trainingandtestdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G1UmkEEIVqt",
        "outputId": "faca6af1-1846-4e66-d9a2-71587171c4e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mglove_embeddings\u001b[0m/                               testdata.manual.2009.06.14.csv\n",
            "logistic_regression_model_with_cleaning.pkl     tfidf_vectorizer.pkl\n",
            "logistic_regression_model_without_cleaning.pkl  tfidf_vectorizer_with_cleaning.pkl\n",
            "lr_model.pkl                                    tfidf_vectorizer_without_cleaning.pkl\n",
            "svc_model.pkl                                   training.1600000.processed.noemoticon.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and test datasets\n",
        "train_file_path = 'training.1600000.processed.noemoticon.csv'\n",
        "test_file_path = 'testdata.manual.2009.06.14.csv'\n",
        "\n",
        "# The training dataset does not have a header\n",
        "train_columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "train_data = pd.read_csv(train_file_path, names=train_columns, encoding='latin1')\n",
        "\n",
        "# The test dataset does not have a header\n",
        "test_columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
        "test_data = pd.read_csv(test_file_path, names=test_columns, encoding='latin1')\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"Training Data:\")\n",
        "print(train_data.head())\n",
        "\n",
        "print(\"\\nTest Data:\")\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yItwcwQMIoJj",
        "outputId": "0a1b33e8-8d68-4307-8b22-6e124f5d8eef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "   target         ids                          date      flag  \\\n",
            "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "              user                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "\n",
            "Test Data:\n",
            "   target  ids                          date     flag      user  \\\n",
            "0       4    3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
            "1       4    4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
            "2       4    5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
            "3       4    6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
            "4       4    7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
            "\n",
            "                                                text  \n",
            "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
            "1  Reading my kindle2...  Love it... Lee childs i...  \n",
            "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
            "3  @kenburbary You'll love your Kindle2. I've had...  \n",
            "4  @mikefish  Fair enough. But i have the Kindle2...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the text with links and stop words"
      ],
      "metadata": {
        "id": "7NIo3Zo0hoKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def enhanced_clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
        "    text = text.lower()  # Lowercase\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "train_data['cleaned_text'] = train_data['text'].apply(enhanced_clean_text)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(enhanced_clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW_lQ90dY9rT",
        "outputId": "8b26c7de-dbbf-4013-9571-f0fe64508c7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Glove Weights"
      ],
      "metadata": {
        "id": "lNAjn690igKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove.6B/\n",
        "\n",
        "# Load GloVe embeddings\n",
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_file = 'glove.6B/glove.6B.50d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_file)\n"
      ],
      "metadata": {
        "id": "dZOCpVHw2da7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting Glove 50d weights"
      ],
      "metadata": {
        "id": "9zrYXRNCiket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = 'glove_embeddings/glove.6B.50d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_file)"
      ],
      "metadata": {
        "id": "MVKyQCqPZR3N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def get_glove_embeddings(text, embeddings_index, dim=50):\n",
        "    words = text.split()\n",
        "    word_embeddings = [embeddings_index.get(word, np.zeros(dim)) for word in words]\n",
        "    if len(word_embeddings) == 0:\n",
        "        return np.zeros(dim)\n",
        "    else:\n",
        "        return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "X_train_glove = np.array([get_glove_embeddings(text, embeddings_index) for text in train_data['cleaned_text']])\n",
        "X_test_glove = np.array([get_glove_embeddings(text, embeddings_index) for text in test_data['cleaned_text']])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "processed_text = train_data['cleaned_text']\n",
        "sentiment = train_data['target']\n"
      ],
      "metadata": {
        "id": "pGSefnd4Z0Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model"
      ],
      "metadata": {
        "id": "F9CqIjphioE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "X_train, X_test_train, y_train, y_test_train = train_test_split(processed_text, sentiment, test_size=0.05, random_state=0)\n",
        "\n",
        "X_train_glove = np.array([get_glove_embeddings(text, embeddings_index) for text in X_train])\n",
        "X_test_train_glove = np.array([get_glove_embeddings(text, embeddings_index) for text in X_test_train])\n",
        "X_test_glove = np.array([get_glove_embeddings(text, embeddings_index) for text in test_data['cleaned_text']])\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {np.unique(y_train)[i]: class_weights[i] for i in range(len(class_weights))}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Train the model with class weights\n",
        "SVCmodel = LinearSVC(class_weight=class_weight_dict)\n",
        "SVCmodel.fit(X_train_glove, y_train)\n",
        "print(f'Model trained.')\n",
        "\n",
        "# Save the model and vectorizer\n",
        "joblib.dump(SVCmodel, 'svc_model.pkl')\n",
        "print(f'Model saved.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWOvBDoFaPjK",
        "outputId": "9501154a-414d-434b-f49e-a400bf1e159d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: {0: 0.999985526525274, 4: 1.0000144738937011}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained.\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Evaluation"
      ],
      "metadata": {
        "id": "1v2L86DRirET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_Evaluate(model, X_test, y_test):\n",
        "    # Predict values for the test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Evaluate the model on the internal test set\n",
        "print(\"Evaluation on internal test set:\")\n",
        "model_Evaluate(SVCmodel, X_test_train_glove, y_test_train)\n",
        "\n",
        "# Evaluate the model on the external test data\n",
        "print(\"Evaluation on external test data:\")\n",
        "model_Evaluate(SVCmodel, X_test_glove, test_data['target'])\n",
        "\n",
        "# Function to predict sentiment of a single sentence\n",
        "def predict_sentiment(sentence):\n",
        "    # Preprocess the sentence\n",
        "    cleaned_sentence = enhanced_clean_text(sentence)\n",
        "    # Get GloVe embeddings for the sentence\n",
        "    glove_embedding = get_glove_embeddings(cleaned_sentence, embeddings_index)\n",
        "    # Predict the sentiment\n",
        "    prediction = SVCmodel.predict([glove_embedding])\n",
        "    return prediction[0]\n",
        "\n",
        "# Example usage\n",
        "example_sentence = \"I love sunny days but I hate the rain.\"\n",
        "predicted_sentiment = predict_sentiment(example_sentence)\n",
        "print(f\"Predicted sentiment for '{example_sentence}': {predicted_sentiment}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3R63bc7b0vK",
        "outputId": "971c8fbe-1aeb-442f-ef00-f077de138e39"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on internal test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.66      0.67     39989\n",
            "           4       0.67      0.67      0.67     40011\n",
            "\n",
            "    accuracy                           0.67     80000\n",
            "   macro avg       0.67      0.67      0.67     80000\n",
            "weighted avg       0.67      0.67      0.67     80000\n",
            "\n",
            "Evaluation on external test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.71      0.67       177\n",
            "           2       0.00      0.00      0.00       139\n",
            "           4       0.50      0.84      0.63       182\n",
            "\n",
            "    accuracy                           0.56       498\n",
            "   macro avg       0.38      0.51      0.43       498\n",
            "weighted avg       0.41      0.56      0.47       498\n",
            "\n",
            "Predicted sentiment for 'I love sunny days but I hate the rain.': 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVB_Bdzg2P4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_t4nxOR02P6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}